{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizer.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.22.4\n",
      "  Downloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: torch==2.2.2 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib==3.6.2 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: pandas==2.0.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit_learn==1.1.3 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: tqdm==4.66.2 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: setuptools==65.5.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (65.5.0)\n",
      "Requirement already satisfied: sympy==1.11.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scanpy in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: pykan in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.2) (3.17.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.2) (3.1.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dahal\\appdata\\roaming\\python\\python310\\site-packages (from torch==2.2.2) (4.12.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.2) (2024.12.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.2.2) (3.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.6.2) (4.56.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dahal\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib==3.6.2) (24.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.6.2) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.6.2) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dahal\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib==3.6.2) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.6.2) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.6.2) (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.6.2) (1.4.8)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==2.0.1) (2025.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas==2.0.1) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn==1.1.3) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn==1.1.3) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit_learn==1.1.3) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dahal\\appdata\\roaming\\python\\python310\\site-packages (from tqdm==4.66.2) (0.4.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.11.1) (1.3.0)\n",
      "Collecting scanpy\n",
      "  Downloading scanpy-1.10.4-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: legacy-api-wrap>=1.4 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (1.4.1)\n",
      "Requirement already satisfied: patsy!=1.0.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (1.0.1)\n",
      "Collecting session-info\n",
      "  Using cached session_info-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: h5py>=3.6 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (3.13.0)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (0.5.7)\n",
      "Collecting scanpy\n",
      "  Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
      "  Downloading scanpy-1.10.2-py3-none-any.whl (2.1 MB)\n",
      "  Downloading scanpy-1.10.1-py3-none-any.whl (2.1 MB)\n",
      "  Downloading scanpy-1.10.0-py3-none-any.whl (2.1 MB)\n",
      "  Downloading scanpy-1.9.8-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: anndata>=0.7.4 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (0.11.3)\n",
      "Requirement already satisfied: natsort in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: numba>=0.41.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (0.61.0)\n",
      "Requirement already satisfied: statsmodels>=0.10.0rc2 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scanpy) (0.14.4)\n",
      "Collecting anndata>=0.7.4\n",
      "  Downloading anndata-0.11.2-py3-none-any.whl (142 kB)\n",
      "  Downloading anndata-0.11.1-py3-none-any.whl (141 kB)\n",
      "  Downloading anndata-0.11.0-py3-none-any.whl (141 kB)\n",
      "  Downloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
      "  Downloading anndata-0.10.8-py3-none-any.whl (124 kB)\n",
      "  Downloading anndata-0.10.7-py3-none-any.whl (122 kB)\n",
      "  Downloading anndata-0.10.6-py3-none-any.whl (122 kB)\n",
      "  Downloading anndata-0.10.5.post1-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\dahal\\appdata\\roaming\\python\\python310\\site-packages (from anndata>=0.7.4->scanpy) (1.2.2)\n",
      "Requirement already satisfied: array-api-compat in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anndata>=0.7.4->scanpy) (1.10.0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp310-cp310-win_amd64.whl (216 kB)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-win_amd64.whl (187 kB)\n",
      "Collecting numba>=0.41.0\n",
      "  Downloading numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dahal\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib==3.6.2) (1.17.0)\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached scipy-1.15.1-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "  Downloading scipy-1.15.0-cp310-cp310-win_amd64.whl (43.9 MB)\n",
      "  Downloading scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "  Downloading scipy-1.14.0-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "Requirement already satisfied: pynndescent>=0.5 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from umap-learn!=0.5.0,>=0.5->scanpy) (0.5.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dahal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
      "Collecting stdlib-list\n",
      "  Downloading stdlib_list-0.11.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: numpy, scipy, llvmlite, numba, contourpy, stdlib-list, session-info, anndata, scanpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.2\n",
      "    Uninstalling scipy-1.15.2:\n",
      "      Successfully uninstalled scipy-1.15.2\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.44.0\n",
      "    Uninstalling llvmlite-0.44.0:\n",
      "      Successfully uninstalled llvmlite-0.44.0\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.61.0\n",
      "    Uninstalling numba-0.61.0:\n",
      "      Successfully uninstalled numba-0.61.0\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.3.1\n",
      "    Uninstalling contourpy-1.3.1:\n",
      "      Successfully uninstalled contourpy-1.3.1\n",
      "  Attempting uninstall: anndata\n",
      "    Found existing installation: anndata 0.11.3\n",
      "    Uninstalling anndata-0.11.3:\n",
      "      Successfully uninstalled anndata-0.11.3\n",
      "  Attempting uninstall: scanpy\n",
      "    Found existing installation: scanpy 1.11.0\n",
      "    Uninstalling scanpy-1.11.0:\n",
      "      Successfully uninstalled scanpy-1.11.0\n",
      "Successfully installed anndata-0.10.5.post1 contourpy-1.2.1 llvmlite-0.43.0 numba-0.60.0 numpy-1.22.4 scanpy-1.9.8 scipy-1.13.1 session-info-1.0.0 stdlib-list-0.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\dahal\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.22.4 torch==2.2.2 matplotlib==3.6.2 pandas==2.0.1 scikit_learn==1.1.3 tqdm==4.66.2 setuptools==65.5.0 sympy==1.11.1 seaborn scanpy pykan pyyaml\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import psutil\n",
    "\n",
    "\n",
    "def configure_system():\n",
    "    \"\"\"Configure system settings for optimal performance.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    torch.set_num_threads(min(multiprocessing.cpu_count(), 8))\n",
    "\n",
    "\n",
    "def get_optimal_batch_size(total_samples):\n",
    "    \"\"\"\n",
    "    Determine optimal batch size based on system and data.\n",
    "\n",
    "    Args:\n",
    "        total_samples: Total number of samples in dataset\n",
    "\n",
    "    Returns:\n",
    "        int: Optimal batch size\n",
    "    \"\"\"\n",
    "    return 256 if torch.cuda.is_available() else min(64, total_samples // 100)\n",
    "\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed: Random seed value\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"\n",
    "    Get system configuration information.\n",
    "\n",
    "    Returns:\n",
    "        dict: System information\n",
    "    \"\"\"\n",
    "    info = {\n",
    "        \"cpu_cores\": multiprocessing.cpu_count(),\n",
    "        \"ram_gb\": psutil.virtual_memory().total / (1024**3),\n",
    "    }\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        info.update(\n",
    "            {\n",
    "                \"gpu\": torch.cuda.get_device_name(0),\n",
    "                \"gpu_memory_gb\": torch.cuda.get_device_properties(0).total_memory\n",
    "                / (1024**3),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processor.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "\n",
    "class KANDataProcessor1:\n",
    "    \"\"\"Processes gene expression data for KAN model training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gene_data = {}\n",
    "        self.sample_names = None\n",
    "        self.related_genes = {}\n",
    "\n",
    "    def prepare_training_data(\n",
    "        self, expression_file: Path, network_file: Path, target_gene: str\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Prepares training data for a target gene.\n",
    "\n",
    "        Args:\n",
    "            expression_file: Path to h5ad expression data file\n",
    "            network_file: Path to network TSV file\n",
    "            target_gene: Name of target gene\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (input matrix, target values, model config)\n",
    "        \"\"\"\n",
    "        # Load and process data\n",
    "        adata = sc.read_h5ad(expression_file)\n",
    "        expr_matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "        gene_names = adata.var_names.tolist()\n",
    "        self.sample_names = adata.obs_names.tolist()\n",
    "\n",
    "        # Get related genes from network\n",
    "        network_df = pd.read_csv(network_file, sep=\"\\t\")\n",
    "        related_genes = self._get_related_genes(network_df, target_gene, gene_names)\n",
    "        self.related_genes[target_gene] = related_genes\n",
    "\n",
    "        # Prepare input matrix and target vector\n",
    "        X, y = self._prepare_matrices(\n",
    "            expr_matrix, gene_names, target_gene, related_genes\n",
    "        )\n",
    "\n",
    "        # Create model config\n",
    "        config = {\n",
    "            \"width\": [X.shape[1], 1, 1],\n",
    "            # \"width\": X.shape[1],\n",
    "            \"grid\": 3,\n",
    "            \"k\": 3,\n",
    "            \"seed\": 42,\n",
    "            # \"feature_names\": related_genes,\n",
    "        }\n",
    "        # config[\"feature_names\"] = related_genes\n",
    "        return X.astype(np.float32), y.astype(np.float32), config\n",
    "\n",
    "    def _get_related_genes(\n",
    "        self, network_df: pd.DataFrame, target_gene: str, gene_names: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Gets list of genes related to target gene from network.\"\"\"\n",
    "        source_col, target_col = network_df.columns[:2]\n",
    "        related = network_df[\n",
    "            (network_df[source_col] == target_gene)\n",
    "            | (network_df[target_col] == target_gene)\n",
    "        ]\n",
    "\n",
    "        genes = []\n",
    "        for _, row in related.iterrows():\n",
    "            gene = (\n",
    "                row[target_col] if row[source_col] == target_gene else row[source_col]\n",
    "            )\n",
    "            if gene in gene_names:\n",
    "                genes.append(gene)\n",
    "\n",
    "        return genes\n",
    "\n",
    "    def _prepare_matrices(\n",
    "        self,\n",
    "        expr_matrix: np.ndarray,\n",
    "        gene_names: List[str],\n",
    "        target_gene: str,\n",
    "        related_genes: List[str],\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepares input and target matrices.\"\"\"\n",
    "        target_idx = gene_names.index(target_gene)\n",
    "        related_indices = [gene_names.index(gene) for gene in related_genes]\n",
    "\n",
    "        X = expr_matrix[:, related_indices]\n",
    "        y = expr_matrix[:, target_idx]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_related_genes(self, target_gene: str) -> List[str]:\n",
    "        \"\"\"Returns list of genes related to target gene.\"\"\"\n",
    "        return self.related_genes.get(target_gene, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.39712295,  1.0010471 , -0.31169292, ..., -0.3018383 ,\n",
       "          1.43981   , -0.08644036],\n",
       "        [-0.39712295,  0.0065413 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        [-0.39712295,  1.1147025 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        ...,\n",
       "        [-0.39712295, -1.2648143 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        [-0.39712295,  0.89651644, -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        [-0.39712295,  0.7615987 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036]], dtype=float32),\n",
       " array([ 0.9646256 ,  0.7348374 ,  0.8462327 , ..., -1.2689325 ,\n",
       "         1.258737  , -0.02595734], dtype=float32),\n",
       " {'width': [18, 1, 1], 'grid': 3, 'k': 3, 'seed': 42})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=KANDataProcessor1()\n",
    "y=a.prepare_training_data(Path(\"Data/expression_data1.h5ad\"), Path(\"Data/JUN_interactions.tsv\"), \"JUN\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kan\n",
    "from kan import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class KANDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def prepare_data(X, y, batch_size=32):\n",
    "    # Split indices\n",
    "    indices = np.random.permutation(len(X))\n",
    "    train_size = int(0.8 * len(X))\n",
    "    val_size = int(0.1 * len(X))\n",
    "\n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size : train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size :]\n",
    "\n",
    "    # Create datasets\n",
    "    train_loader = DataLoader(\n",
    "        KANDataset(X[train_idx], y[train_idx]), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(KANDataset(X[val_idx], y[val_idx]), batch_size=batch_size)\n",
    "    test_loader = DataLoader(\n",
    "        KANDataset(X[test_idx], y[test_idx]), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Epoch 0: Train Loss = 0.9392, Val Loss = 0.8810\n",
      "Epoch 1: Train Loss = 0.8418, Val Loss = 0.7578\n",
      "Epoch 2: Train Loss = 0.7224, Val Loss = 0.6322\n",
      "Epoch 3: Train Loss = 0.6256, Val Loss = 0.5508\n",
      "Epoch 4: Train Loss = 0.5668, Val Loss = 0.5053\n",
      "Epoch 5: Train Loss = 0.5329, Val Loss = 0.4792\n",
      "Epoch 6: Train Loss = 0.5115, Val Loss = 0.4620\n",
      "Epoch 7: Train Loss = 0.4960, Val Loss = 0.4491\n",
      "Epoch 8: Train Loss = 0.4841, Val Loss = 0.4393\n",
      "Epoch 9: Train Loss = 0.4746, Val Loss = 0.4317\n",
      "Epoch 10: Train Loss = 0.4673, Val Loss = 0.4261\n",
      "Epoch 11: Train Loss = 0.4616, Val Loss = 0.4221\n",
      "Epoch 12: Train Loss = 0.4574, Val Loss = 0.4193\n",
      "Epoch 13: Train Loss = 0.4543, Val Loss = 0.4175\n",
      "Epoch 14: Train Loss = 0.4520, Val Loss = 0.4161\n",
      "Epoch 15: Train Loss = 0.4501, Val Loss = 0.4153\n",
      "Epoch 16: Train Loss = 0.4488, Val Loss = 0.4147\n",
      "Epoch 17: Train Loss = 0.4478, Val Loss = 0.4142\n",
      "Epoch 18: Train Loss = 0.4469, Val Loss = 0.4141\n",
      "Epoch 19: Train Loss = 0.4462, Val Loss = 0.4137\n",
      "Epoch 20: Train Loss = 0.4457, Val Loss = 0.4136\n",
      "Epoch 21: Train Loss = 0.4452, Val Loss = 0.4135\n",
      "Epoch 22: Train Loss = 0.4448, Val Loss = 0.4134\n",
      "Epoch 23: Train Loss = 0.4445, Val Loss = 0.4133\n",
      "Epoch 24: Train Loss = 0.4442, Val Loss = 0.4132\n",
      "Epoch 25: Train Loss = 0.4439, Val Loss = 0.4132\n",
      "Epoch 26: Train Loss = 0.4437, Val Loss = 0.4130\n"
     ]
    }
   ],
   "source": [
    "# Unpack the data\n",
    "X_data, y_target, config = y\n",
    "\n",
    "# Create KAN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KAN(\n",
    "    width=config[\"width\"],\n",
    "    grid=config[\"grid\"],\n",
    "    k=config[\"k\"],\n",
    "    seed=config[\"seed\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Split and create dataloaders\n",
    "train_loader, val_loader, test_loader = prepare_data(X_data, y_target)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = sum(\n",
    "            train_batch(model, X_batch, y_batch, optimizer, criterion)\n",
    "            for X_batch, y_batch in train_loader\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = sum(\n",
    "                criterion(model(X.to(device)).squeeze(), y.to(device)).item()\n",
    "                for X, y in val_loader\n",
    "            )\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        history.append((train_loss, val_loss))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "            break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_batch(model, X, y, optimizer, criterion):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output.squeeze(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Usage\n",
    "history = train_with_early_stopping(\n",
    "    model, train_loader, val_loader, optimizer, criterion, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.auto_symbolic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.symbolic_formula()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kan_feature_contributions(model, X_tensor, feature_names):\n",
    "    model.eval()\n",
    "    contributions = {}\n",
    "\n",
    "    weights = model.state_dict()\n",
    "    print(\"Weights shapes:\")\n",
    "    print(\"act_weights shape:\", weights[\"act_fun.0.coef\"].shape)\n",
    "    print(\"sym_weights shape:\", weights[\"symbolic_fun.0.affine\"].shape)\n",
    "    print(\"X_tensor shape:\", X_tensor.shape)\n",
    "    print(\"Number of features:\", len(feature_names))\n",
    "\n",
    "    act_weights = weights[\"act_fun.0.coef\"].cpu().numpy()   #Non-linear contribution\n",
    "    sym_weights = weights[\"symbolic_fun.0.affine\"].cpu().numpy() #Linear contribution\n",
    "\n",
    "    for i in range(X_tensor.shape[1]):\n",
    "        w_impact = np.abs(sym_weights[0, i, :]).mean()  # Adjusted for shape (1, 18, 4)\n",
    "        a_impact = np.abs(act_weights[i, :, :]).mean()  # Adjusted for shape (18, 1, 7)\n",
    "        contributions[feature_names[i]] = float(w_impact + a_impact)\n",
    "\n",
    "    # Normalize and filter\n",
    "    total = sum(contributions.values())\n",
    "    contributions = {k: v / total for k, v in contributions.items() if v / total > 0.001}\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    names = list(contributions.keys())\n",
    "    values = list(contributions.values())\n",
    "    plt.bar(names, values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Feature Contributions Based on Model Weights\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dict(sorted(contributions.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "network_df = pd.read_csv(\"Data/JUN_interactions.tsv\", sep=\"\\t\")\n",
    "adata = sc.read_h5ad(\"Data/expression_data1.h5ad\")\n",
    "gene_names = adata.var_names.tolist()\n",
    "\n",
    "# Get related genes\n",
    "a = KANDataProcessor1()\n",
    "X_data, y_target, config = a.prepare_training_data(\n",
    "    Path(\"Data/expression_data1.h5ad\"), Path(\"Data/JUN_interactions.tsv\"), \"JUN\"\n",
    ")\n",
    "related_genes = a.get_related_genes(\"JUN\")\n",
    "\n",
    "# Create input tensor\n",
    "X_tensor = torch.FloatTensor(X_data).to(device)\n",
    "\n",
    "\n",
    "# Load model with correct map_location\n",
    "checkpoint = torch.load(\"best_model.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Then move model to device\n",
    "model = model.to(device)\n",
    "X_tensor = X_tensor.to(device)\n",
    "contributions = analyze_kan_feature_contributions(model, X_tensor, related_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
