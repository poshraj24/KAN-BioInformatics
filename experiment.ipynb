{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the necessary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy==1.22.4 torch==2.2.2 matplotlib==3.6.2 pandas==2.0.1 scikit_learn==1.1.3 tqdm==4.66.2 setuptools==65.5.0 sympy==1.11.1 seaborn scanpy pykan pyyaml\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing and Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class processes gene expression data for KAN (Kolmogorov-Arnold Network) model training. It extracts related genes from interaction network files, prepares input matrices by selecting only relevant gene expression data, and configures model parameters. The `prepare_training_data` method creates a complete pipeline that takes expression data (h5ad) and network files as input, then returns processed feature matrices, target values, and model configuration needed for training a gene expression prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "\n",
    "class KANDataProcessor1:\n",
    "    \"\"\"Processes gene expression data for KAN model training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gene_data = {}\n",
    "        self.sample_names = None\n",
    "        self.related_genes = {}\n",
    "\n",
    "    def prepare_training_data(\n",
    "        self, expression_file: Path, network_file: Path, target_gene: str\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Prepares training data for a target gene.\n",
    "\n",
    "        Args:\n",
    "            expression_file: Path to h5ad expression data file\n",
    "            network_file: Path to network TSV file\n",
    "            target_gene: Name of target gene\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (input matrix, target values, model config)\n",
    "        \"\"\"\n",
    "        # Load and process data\n",
    "        adata = sc.read_h5ad(expression_file)\n",
    "        expr_matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "        gene_names = adata.var_names.tolist()\n",
    "        self.sample_names = adata.obs_names.tolist()\n",
    "\n",
    "        # Get related genes from network\n",
    "        network_df = pd.read_csv(network_file, sep=\"\\t\")\n",
    "        related_genes = self._get_related_genes(network_df, target_gene, gene_names)\n",
    "        self.related_genes[target_gene] = related_genes\n",
    "\n",
    "        # Prepare input matrix and target vector\n",
    "        X, y = self._prepare_matrices(\n",
    "            expr_matrix, gene_names, target_gene, related_genes\n",
    "        )\n",
    "\n",
    "        # Create model config\n",
    "        config = {\n",
    "            \"width\": [X.shape[1], 1, 1],\n",
    "            # \"width\": X.shape[1],\n",
    "            \"grid\": 5,\n",
    "            \"k\": 4,\n",
    "            \"seed\": 42,\n",
    "            # \"feature_names\": related_genes,\n",
    "        }\n",
    "        # config[\"feature_names\"] = related_genes\n",
    "        return X.astype(np.float32), y.astype(np.float32), config\n",
    "\n",
    "    def _get_related_genes(\n",
    "        self, network_df: pd.DataFrame, target_gene: str, gene_names: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Gets list of genes related to target gene from network.\"\"\"\n",
    "        source_col, target_col = network_df.columns[:2]\n",
    "        related = network_df[\n",
    "            (network_df[source_col] == target_gene)\n",
    "            | (network_df[target_col] == target_gene)\n",
    "        ]\n",
    "\n",
    "        genes = []\n",
    "        for _, row in related.iterrows():\n",
    "            gene = (\n",
    "                row[target_col] if row[source_col] == target_gene else row[source_col]\n",
    "            )\n",
    "            if gene in gene_names:\n",
    "                genes.append(gene)\n",
    "\n",
    "        return genes\n",
    "\n",
    "    def _prepare_matrices(\n",
    "        self,\n",
    "        expr_matrix: np.ndarray,\n",
    "        gene_names: List[str],\n",
    "        target_gene: str,\n",
    "        related_genes: List[str],\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepares input and target matrices.\"\"\"\n",
    "        target_idx = gene_names.index(target_gene)\n",
    "        related_indices = [gene_names.index(gene) for gene in related_genes]\n",
    "\n",
    "        X = expr_matrix[:, related_indices]\n",
    "        y = expr_matrix[:, target_idx]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def get_related_genes(self, target_gene: str) -> List[str]:\n",
    "        \"\"\"Returns list of genes related to target gene.\"\"\"\n",
    "        return self.related_genes.get(target_gene, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the result of preparing training data for a KAN model designed to predict JUN gene expression. It contains three elements:\n",
    "\n",
    "1. A feature matrix (X) with dimensions [17041, 18] containing expression values for 18 related genes identified from the JUN interaction network. The data has been transformed to float32 type.\n",
    "\n",
    "2. A target vector (y) containing the expression values for the JUN gene across all samples.\n",
    "\n",
    "3. A configuration dictionary for the KAN model with parameters:\n",
    "   - width: [18, 2, 1] (input dimension, 1-hidden layer with 2 nodes, output)\n",
    "   - grid: 5 (resolution parameter for the KAN)\n",
    "   - k: 4 (complexity parameter)\n",
    "   - seed: 42 (for reproducibility)\n",
    "\n",
    "This prepared dataset will be used to train the KAN model to learn the complex relationships between JUN and its interacting genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.39712295,  1.0010471 , -0.31169292, ..., -0.3018383 ,\n",
       "          1.43981   , -0.08644036],\n",
       "        [-0.39712295,  0.0065413 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        [-0.39712295,  1.1147025 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        ...,\n",
       "        [-0.39712295, -1.2648143 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        [-0.39712295,  0.89651644, -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036],\n",
       "        [-0.39712295,  0.7615987 , -0.31169292, ..., -0.3018383 ,\n",
       "         -0.32482678, -0.08644036]], dtype=float32),\n",
       " array([ 0.9646256 ,  0.7348374 ,  0.8462327 , ..., -1.2689325 ,\n",
       "         1.258737  , -0.02595734], dtype=float32),\n",
       " {'width': [18, 1, 1], 'grid': 5, 'k': 4, 'seed': 42})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=KANDataProcessor1()\n",
    "y=a.prepare_training_data(Path(\"Data/expression_data1.h5ad\"), Path(\"Data/JUN_interactions.tsv\"), \"JUN\")\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kan\n",
    "from kan import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The `KANDataset` class wraps gene expression data in a PyTorch-compatible format, converting NumPy arrays to tensors and providing standard Dataset functionality.\n",
    "\n",
    "2. The `prepare_data` function splits the data into train (80%), validation (10%), and test (10%) sets using random permutation, then creates DataLoader objects for each set with appropriate batching and shuffling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class KANDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def prepare_data(X, y, batch_size=32):\n",
    "    # Split indices\n",
    "    indices = np.random.permutation(len(X))\n",
    "    train_size = int(0.8 * len(X))\n",
    "    val_size = int(0.1 * len(X))\n",
    "\n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size : train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size :]\n",
    "\n",
    "    # Create datasets\n",
    "    train_loader = DataLoader(\n",
    "        KANDataset(X[train_idx], y[train_idx]), batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(KANDataset(X[val_idx], y[val_idx]), batch_size=batch_size)\n",
    "    test_loader = DataLoader(\n",
    "        KANDataset(X[test_idx], y[test_idx]), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and train a KAN (Kolmogorov-Arnold Network) model to predict JUN gene expression from related genes. It:\n",
    "\n",
    "1. Initializes the model with the configuration obtained from data preprocessing\n",
    "2. Creates a training pipeline with early stopping, tracking multiple performance metrics (loss, R², RMSE, MAE)\n",
    "3. Implements custom training and evaluation functions with proper device handling for GPU acceleration\n",
    "4. Visualizes training progress with multiple metrics plotted over time\n",
    "\n",
    "The implementation uses AdamW optimizer with a learning rate of 0.0001 and weight decay of 0.01. Early stopping monitors validation loss with patience=5 to prevent overfitting. The training successfully completes after about 69 epochs when early stopping triggers, saving the best model based on validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|▌| 59/100 [25:16<17:27, 25.55s/it, train_loss=0.4392, test_loss=0.4282, val_loss=0.41"
     ]
    }
   ],
   "source": [
    "# Unpack the data\n",
    "X_data, y_target, config = y\n",
    "\n",
    "# Create KAN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KAN(\n",
    "    width=config[\"width\"],\n",
    "    grid=config[\"grid\"],\n",
    "    k=config[\"k\"],\n",
    "    seed=config[\"seed\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Split and create dataloaders\n",
    "train_loader, val_loader, test_loader = prepare_data(X_data, y_target)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    total_sum_squares = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    residual_sum_squares = torch.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (residual_sum_squares / total_sum_squares)\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs,\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training\", ncols=100, position=0, leave=True)\n",
    "    for epochs in pbar:\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_r2, train_rmse, train_mae = 0, 0, 0, 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            loss = train_batch(model, X_batch, y_batch, optimizer, criterion)\n",
    "            train_loss += loss\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_r2, val_rmse, val_mae = 0, 0, 0, 0\n",
    "        test_loss, test_r2, test_rmse, test_mae = 0, 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                predictions = model(X.to(device)).squeeze()\n",
    "                y = y.to(device)\n",
    "                val_loss += criterion(predictions, y).item()\n",
    "                val_r2 += r2_score(y, predictions)\n",
    "                val_rmse += rmse(y, predictions)\n",
    "                val_mae += mae(y, predictions)\n",
    "\n",
    "            for X, y in test_loader:\n",
    "                predictions = model(X.to(device)).squeeze()\n",
    "                y = y.to(device)\n",
    "                test_loss += criterion(predictions, y).item()\n",
    "                test_r2 += r2_score(y, predictions)\n",
    "                test_rmse += rmse(y, predictions)\n",
    "                test_mae += mae(y, predictions)\n",
    "\n",
    "        # Normalize metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        test_loss /= len(test_loader)\n",
    "        val_r2 /= len(val_loader)\n",
    "        val_rmse /= len(val_loader)\n",
    "        val_mae /= len(val_loader)\n",
    "        test_r2 /= len(test_loader)\n",
    "        test_rmse /= len(test_loader)\n",
    "        test_mae /= len(test_loader)\n",
    "\n",
    "        # Store metrics\n",
    "        history.append((train_loss, val_loss, test_loss, val_r2,val_rmse, val_mae))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"train_loss\": f\"{train_loss:.4f}\",\n",
    "                \"test_loss\": f\"{test_loss:.4f}\",\n",
    "                \"val_loss\": f\"{val_loss:.4f}\",\n",
    "                \"val_r2\": f\"{val_r2:.4f}\",\n",
    "                \"val_rmse\": f\"{val_rmse:.4f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"\\nEarly stopping triggered\")\n",
    "            model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "            break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_batch(model, X, y, optimizer, criterion):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output.squeeze(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def plot_training_metrics(history):\n",
    "    epochs = np.array(range(1, len(history) + 1))\n",
    "    metrics = [\n",
    "        np.array([x.cpu().numpy() if torch.is_tensor(x) else x for x in m])\n",
    "        for m in zip(*history)\n",
    "    ]\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Loss Plot\n",
    "    ax1.plot(epochs, metrics[0], \"b-\", label=\"Train Loss\")\n",
    "    ax1.plot(epochs, metrics[1], \"r-\", label=\"Val Loss\")\n",
    "    ax1.plot(epochs, metrics[2], \"g-\", label=\"Test Loss\")\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.legend()\n",
    "\n",
    "    # R² Plot\n",
    "    ax2.plot(epochs, metrics[3], \"r-\", label=\"Validation R²\")\n",
    "    ax2.set_title(\"R² Score\")\n",
    "    ax2.legend()\n",
    "\n",
    "    # RMSE Plot\n",
    "    ax3.plot(epochs, metrics[4], \"r-\", label=\"Validation RMSE\")\n",
    "    ax3.set_title(\"RMSE\")\n",
    "    ax3.legend()\n",
    "\n",
    "    # MAE Plot\n",
    "    ax4.plot(epochs, metrics[5], \"r-\", label=\"Validation MAE\")\n",
    "    ax4.set_title(\"MAE\")\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "history = train_with_early_stopping(\n",
    "    model, train_loader, val_loader, test_loader,optimizer, criterion, epochs\n",
    ")\n",
    "plot_training_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set activation functions to be symbolic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.auto_symbolic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the symbolic formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan.utils import ex_round\n",
    "ex_round(model.symbolic_formula()[0][0],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Input Feature Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kan_feature_contributions(model, X_tensor, feature_names):\n",
    "    model.eval()\n",
    "    contributions = {}\n",
    "\n",
    "    weights = model.state_dict()\n",
    "    print(\"Weights shapes:\")\n",
    "    print(\"act_weights shape:\", weights[\"act_fun.0.coef\"].shape)\n",
    "    print(\"sym_weights shape:\", weights[\"symbolic_fun.0.affine\"].shape)\n",
    "    print(\"X_tensor shape:\", X_tensor.shape)\n",
    "    print(\"Number of features:\", len(feature_names))\n",
    "\n",
    "    act_weights = weights[\"act_fun.0.coef\"].cpu().numpy()   #Non-linear contribution\n",
    "    sym_weights = weights[\"symbolic_fun.0.affine\"].cpu().numpy() #Linear contribution\n",
    "\n",
    "    for i in range(X_tensor.shape[1]):\n",
    "        w_impact = np.abs(sym_weights[0, i, :]).mean()  # Adjusted for shape (1, 18, 4)\n",
    "        a_impact = np.abs(act_weights[i, :, :]).mean()  # Adjusted for shape (18, 1, 7)\n",
    "        contributions[feature_names[i]] = float(w_impact + a_impact)\n",
    "\n",
    "    # Normalize and filter\n",
    "    total = sum(contributions.values())\n",
    "    contributions = {k: v / total for k, v in contributions.items() if v / total > 0.001}\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    names = list(contributions.keys())\n",
    "    values = list(contributions.values())\n",
    "    plt.bar(names, values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Feature Contributions Based on Model Weights\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return dict(sorted(contributions.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "network_df = pd.read_csv(\"Data/JUN_interactions.tsv\", sep=\"\\t\")\n",
    "adata = sc.read_h5ad(\"Data/expression_data1.h5ad\")\n",
    "gene_names = adata.var_names.tolist()\n",
    "\n",
    "# Get related genes\n",
    "a = KANDataProcessor1()\n",
    "X_data, y_target, config = a.prepare_training_data(\n",
    "    Path(\"Data/expression_data1.h5ad\"), Path(\"Data/JUN_interactions.tsv\"), \"JUN\"\n",
    ")\n",
    "related_genes = a.get_related_genes(\"JUN\")\n",
    "\n",
    "# Create input tensor\n",
    "X_tensor = torch.FloatTensor(X_data).to(device)\n",
    "\n",
    "\n",
    "# Load model with correct map_location\n",
    "checkpoint = torch.load(\"best_model.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Then move model to device\n",
    "model = model.to(device)\n",
    "X_tensor = X_tensor.to(device)\n",
    "contributions = analyze_kan_feature_contributions(model, X_tensor, related_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the symbolic formula based on input feature contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
